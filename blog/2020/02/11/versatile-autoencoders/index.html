<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="Keywords" content="David Charte,Charte,Francisco David Charte,David Charte Luque,Francisco David Charte Luque,data scientist,machine learning,deep learning">
  <meta name="description" content="Versatile workflow for autoencoders in Keras">
  <meta property="og:title" content="David Charte's personal website" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="David Charte" />
  <meta property="og:description" content="Versatile workflow for autoencoders in Keras" />
  <title>Versatile workflow for autoencoders in Keras - David Charte</title>
  <link rel="stylesheet" href="/assets/stylesheets/normalize.css">
  <link rel="stylesheet" href="https://unpkg.com/sakura.css/css/sakura.css" type="text/css">
  <link rel="stylesheet" href="/assets/stylesheets/default.css">
</head>
<body>
  <aside class="top">
    <a href="/">
      <header>
        
        <img src="https://secure.gravatar.com/avatar/7083d161b041964b75c38376d9dee37a?s=200" class="profile-pic" />
        
        <span class="name">
          David Charte
        </span>
      </header>
    </a>
    <div class="arrow-up"></div>
  </aside>
  
<article class="article lang-es">
  <time datetime="2020-02-11">2020-02-11</time>
  
  <a href="/blog/2020/02/11/versatile-autoencoders/">
    <h1>
        Versatile workflow for autoencoders in Keras
        </h1>
        
  </a>
  
  <h2 id="architecture-definition">Architecture definition</h2>

<p>This code defines the simplest architecture: [input, encoding (2), output]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">enc_dim</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">enc_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">))</span>
<span class="p">])</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">enc_dim</span><span class="p">,))</span>
<span class="p">])</span>
</code></pre></div></div>

<h2 id="definition-of-a-loss-function">Definition of a loss function</h2>

<p>Defining the loss as a function allows us much more flexibility to compute penalties from the encodings, or even other inputs (supervised autoencoders).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">autoencoder_loss</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
    <span class="n">inp</span><span class="p">,</span> <span class="n">enc</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">layers</span>
    <span class="n">rec_loss</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
    <span class="c1"># compute other penalties
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">rec_loss</span> <span class="c1"># + other penalties
</span>    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<h2 id="definition-of-the-loss-layer-and-the-model">Definition of the loss layer and the model</h2>

<p>The loss function is wrapped as the last layer of the autoencoder model and connected to the necessary inputs/outputs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_l</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">autoencoder_loss</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s">"loss"</span><span class="p">)([</span>
    <span class="n">encoder</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">encoder</span><span class="p">.</span><span class="n">output</span><span class="p">,</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">.</span><span class="nb">input</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">([</span><span class="n">encoder</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">target_input</span><span class="p">],</span> <span class="n">loss_l</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="compilation-of-model-with-custom-loss">Compilation of model with custom loss</h2>

<p>Since the last layer (<code class="language-plaintext highlighter-rouge">y_pred</code>) is actually the loss, our custom loss only needs to return that value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">autoencoder</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="p">{</span><span class="s">'loss'</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">},</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"adam"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="fit-model-with-dummy-target">Fit model with dummy target</h2>

<p>Keras will complain if no target is passed, so we just create some dummy targets which will not be used at all.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

  

</article>


  <footer>
    &copy; 2022 David Charte. Shared under
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">
      <i class="icon fa fa-creative-commons fa-lg"></i> BY-SA</a>.
  </footer>
    
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.0.11/css/fork-awesome.min.css" integrity="sha256-MGU/JUq/40CFrfxjXb5pZjpoZmxiP2KuICN5ElLFNd8=" crossorigin="anonymous">
  <link rel="stylesheet" href="/assets/stylesheets/highlight.css">
</body>
</html>
  